\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Contents}{2}{section*.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Acknowledgements}{4}{chapter.1}}
\citation{lecun-convolutional-2010}
\citation{hassabis-2016}
\citation{lecun-data-driven-2014}
\citation{chilimbi-2014}
\citation{huang-2013}
\citation{stanley-2002}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Introduction}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivations}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scope}{5}{section.2.2}}
\citation{yao-1999}
\citation{haykin-1998}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Aims and Objectives}{6}{section.2.3}}
\newlabel{section:aims-and-objectives}{{\M@TitleReference {2.3}{Introduction}}{6}{Aims and Objectives}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{Justifications}{6}{subsection*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Prerequisites}{6}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Structure of the Report}{7}{section.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Terminology}{7}{section.2.6}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Literature Review}{8}{chapter.3}}
\citation{wikipedia-evolutionary-algorithms}
\citation{haupt-1998}
\citation{negnevitsky-2011}
\citation{haupt-1998}
\citation{negnevitsky-2011}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Evolutionary Algorithms Theory}{9}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{Background}{9}{subsection*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Canonical Genetical Algorithm. Sources: \cite  {haupt-1998} and \cite  {negnevitsky-2011}.\relax }}{9}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GA-flowchart}{{\M@TitleReference {3.1}{Canonical Genetical Algorithm. Sources: \cite  {haupt-1998} and \cite  {negnevitsky-2011}.\relax }}{9}{Canonical Genetical Algorithm. Sources: \cite {haupt-1998} and \cite {negnevitsky-2011}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{Logic description}{9}{subsection*.5}}
\@writefile{toc}{\contentsline {subsection}{Search Space Behaviour}{9}{subsection*.6}}
\citation{wiki-artificial-neuron}
\citation{wiki-artificial-neuron}
\citation{negnevitsky-2011}
\citation{sibi-2013}
\citation{waghulde-2014}
\citation{coursera-machine-learning-stanford}
\citation{coursera-machine-learning-stanford}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural Network Theory}{10}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Representation of a single artificial neuron. Source: \cite  {wiki-artificial-neuron}\relax }}{10}{figure.caption.7}}
\newlabel{single-neuron-diagram}{{\M@TitleReference {3.2}{Representation of a single artificial neuron. Source: \cite  {wiki-artificial-neuron}\relax }}{10}{Representation of a single artificial neuron. Source: \cite {wiki-artificial-neuron}\relax }{figure.caption.7}{}}
\newlabel{single-neuron-equation}{{3.1}{10}{Neural Network Theory}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{Choice of the activation function}{10}{subsection*.8}}
\@writefile{toc}{\contentsline {subsection}{Artificial Neural Networks}{10}{subsection*.9}}
\newlabel{sec:artificial-neural-network}{{\M@TitleReference {3.2}{Literature Review}}{10}{Artificial Neural Networks}{subsection*.9}{}}
\citation{welch-labs}
\citation{welch-labs}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Example of a fully connected ANN with two hidden layers and bias units (+1). Source: \cite  {coursera-machine-learning-stanford}\relax }}{11}{figure.caption.10}}
\newlabel{neural-network-example}{{\M@TitleReference {3.3}{Example of a fully connected ANN with two hidden layers and bias units (+1). Source: \cite  {coursera-machine-learning-stanford}\relax }}{11}{Example of a fully connected ANN with two hidden layers and bias units (+1). Source: \cite {coursera-machine-learning-stanford}\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example}{11}{subsubsection*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Student data set\relax }}{11}{figure.caption.12}}
\newlabel{matrix-dataset}{{\M@TitleReference {3.4}{Student data set\relax }}{11}{Student data set\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Computation of the content of the hidden layer using the input layer values and the first weight matrix.\relax }}{12}{figure.caption.13}}
\newlabel{typical-matrix-calculation}{{\M@TitleReference {3.5}{Computation of the content of the hidden layer using the input layer values and the first weight matrix.\relax }}{12}{Computation of the content of the hidden layer using the input layer values and the first weight matrix.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example of a Forward Propagation}{12}{subsubsection*.14}}
\newlabel{forward-prop-example}{{\M@TitleReference {3.2}{Literature Review}}{12}{Example of a Forward Propagation}{subsubsection*.14}{}}
\newlabel{neural-network-activation}{{3.2}{12}{Example of a Forward Propagation}{equation.3.2.2}{}}
\citation{haykin-1998}
\citation{tereza-2006}
\citation{coursera-machine-learning-stanford}
\citation{stanley-2002}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Optimization Algorithms Theory}{13}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{Gradient Descent with Backpropagation}{13}{subsection*.15}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent Pseudocode\relax }}{13}{figure.caption.16}}
\newlabel{pseudocode-gradient-descent}{{\M@TitleReference {1}{Gradient Descent Pseudocode\relax }}{13}{Gradient Descent Pseudocode\relax }{figure.caption.16}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Differential Evolution Pseudocode\relax }}{13}{figure.caption.16}}
\newlabel{pseudocode-differential-evolution}{{\M@TitleReference {2}{Differential Evolution Pseudocode\relax }}{13}{Differential Evolution Pseudocode\relax }{figure.caption.16}{}}
\newlabel{equations:mutation-schemes}{{3.3}{13}{Gradient Descent with Backpropagation}{equation.3.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Descent}{13}{subsubsection*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{13}{subsubsection*.18}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution}{13}{subsection*.19}}
\citation{eberhart-1995}
\@writefile{toc}{\contentsline {subsection}{Particle Swarm Optimization}{14}{subsection*.20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Particle Swarm Optimization Pseudocode\relax }}{14}{algorithm.3}}
\newlabel{pseudocode-PSO}{{\M@TitleReference {3}{Particle Swarm Optimization Pseudocode\relax }}{14}{Particle Swarm Optimization Pseudocode\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Preventing premature convergence}{14}{subsubsection*.21}}
\newlabel{velocity-formula}{{3.5}{14}{Preventing premature convergence}{equation.3.3.5}{}}
\newlabel{pso-convergence-rule}{{3.6}{14}{Preventing premature convergence}{equation.3.3.6}{}}
\citation{deeplearning4j}
\citation{yao-1999}
\citation{schaffer-1990}
\citation{tereza-2006}
\citation{zhang-2000}
\citation{waghulde-2014}
\@writefile{toc}{\contentsline {subsection}{Known facts about ANN optimization}{15}{subsection*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Small learning rate: slow convergence.\relax }}{15}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Large learning rate: divergence.\relax }}{15}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Intuitions behind tuning the learning rate on a convex cost function output.\relax }}{15}{figure.caption.23}}
\newlabel{learning-rates-examples}{{\M@TitleReference {3.8}{Intuitions behind tuning the learning rate on a convex cost function output.\relax }}{15}{Intuitions behind tuning the learning rate on a convex cost function output.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Note on Neural Network Ensembles}{15}{section.3.4}}
\citation{coursera-machine-learning-stanford}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Methodology and Design}{16}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Implementation Options}{16}{section.4.1}}
\newlabel{listing:forward-propagation-code}{{\M@TitleReference {4.1}{Methodology and Design}}{16}{Vectorized implementation of the forward propagation}{lstlisting.4.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Vectorized implementation of the forward propagation.}{16}{lstlisting.4.1}}
\citation{coursera-machine-learning-stanford}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Implementation Decisions}{17}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experimental Design}{17}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{Recorded Characteristics}{17}{subsection*.24}}
\@writefile{toc}{\contentsline {subsection}{Data Set Segmentation}{17}{subsection*.25}}
\citation{schaffer-1990}
\citation{floreano-2008}
\citation{yao-1999}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Neural Network Encoding}{18}{section.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Fitness Function}{18}{section.4.5}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error}{18}{subsubsection*.26}}
\newlabel{MSE}{{4.1}{18}{Mean Squared Error}{equation.4.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{Fitness function for Imbalanced Data}{18}{subsection*.27}}
\@writefile{toc}{\contentsline {subsubsection}{Precision and Recall}{19}{subsubsection*.28}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Concepts of true false positives negatives\relax }}{19}{table.caption.29}}
\newlabel{table:true_pos}{{\M@TitleReference {4.1}{Concepts of true false positives negatives\relax }}{19}{Concepts of true false positives negatives\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{F1 score}{19}{subsection*.30}}
\@writefile{toc}{\contentsline {subsection}{Matthews correlation coefficient}{19}{subsection*.31}}
\@writefile{toc}{\contentsline {subsection}{The Crossover Problem}{19}{subsection*.32}}
\citation{yao-1999}
\citation{stanley-2002}
\citation{bishop-1995}
\@writefile{toc}{\contentsline {subsection}{Encoding decisions}{20}{subsection*.33}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces model used for chromosomes: Direct encoding. (One dimensional array containing a topology description followed by the flattened weight matrices).\relax }}{20}{table.caption.34}}
\newlabel{table:genome-encoding}{{\M@TitleReference {4.2}{model used for chromosomes: Direct encoding. (One dimensional array containing a topology description followed by the flattened weight matrices).\relax }}{20}{model used for chromosomes: Direct encoding. (One dimensional array containing a topology description followed by the flattened weight matrices).\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Example Real Number encoding (5 hidden units, 1 hidden layer).\relax }}{20}{table.caption.35}}
\newlabel{table:genome-encoding-example}{{\M@TitleReference {4.3}{Example Real Number encoding (5 hidden units, 1 hidden layer).\relax }}{20}{Example Real Number encoding (5 hidden units, 1 hidden layer).\relax }{table.caption.35}{}}
\newlabel{quote-yao-binary-encoding}{{\M@TitleReference {4.5}{Methodology and Design}}{20}{Encoding decisions}{table.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}The Overfitting Problem}{20}{section.4.6}}
\newlabel{section-the-overfitting-problem}{{\M@TitleReference {4.6}{Methodology and Design}}{20}{The Overfitting Problem}{section.4.6}{}}
\citation{fortmann-2012}
\citation{fortmann-2012}
\citation{mccormick-2013}
\citation{mccormick-2013}
\citation{yao-1999}
\@writefile{toc}{\contentsline {subsection}{Neural Network k-fold Cross-Validation}{21}{subsection*.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Bias and Variance contributing to total error. Source: Scott Fortmann Roe\cite  {fortmann-2012}\relax }}{21}{figure.caption.37}}
\newlabel{bias-variance-tradeoff}{{\M@TitleReference {4.1}{Bias and Variance contributing to total error. Source: Scott Fortmann Roe\cite  {fortmann-2012}\relax }}{21}{Bias and Variance contributing to total error. Source: Scott Fortmann Roe\cite {fortmann-2012}\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Maintaining the positive/negative example ratio}{21}{subsubsection*.38}}
\@writefile{toc}{\contentsline {subsection}{Neural Network Regularization}{21}{subsection*.40}}
\citation{yaochu-2004}
\citation{stanley-2002}
\citation{stanley-2002}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of the 10 fold cross-validation method. Source Chris Mccormick\cite  {mccormick-2013}\relax }}{22}{figure.caption.39}}
\newlabel{figure-cross-validation}{{\M@TitleReference {4.2}{Illustration of the 10 fold cross-validation method. Source Chris Mccormick\cite  {mccormick-2013}\relax }}{22}{Illustration of the 10 fold cross-validation method. Source Chris Mccormick\cite {mccormick-2013}\relax }{figure.caption.39}{}}
\newlabel{error-function-model}{{4.5}{22}{Neural Network Regularization}{equation.4.6.5}{}}
\newlabel{weight-decay}{{4.6}{22}{Neural Network Regularization}{equation.4.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}The Competing Conventions Problem}{22}{section.4.7}}
\citation{coursera-machine-learning-stanford}
\citation{coursera-machine-learning-stanford}
\citation{storn-1997}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Data Preprocessing}{23}{section.4.8}}
\@writefile{toc}{\contentsline {subsection}{Feature Scaling}{23}{subsection*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Mean-normalization formula\relax }}{23}{figure.caption.42}}
\newlabel{mean-norm-formula}{{\M@TitleReference {4.3}{Mean-normalization formula\relax }}{23}{Mean-normalization formula\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Selection of Control Variables of DE}{23}{section.4.9}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Results}{24}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Breast Cancer: Malignant or Benign?}{25}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/RAND)}{25}{subsection*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Optimization of the neural network population (DE-rand)\relax }}{25}{figure.caption.44}}
\newlabel{malignant-perfs-DE-rand}{{\M@TitleReference {5.1}{Optimization of the neural network population (DE-rand)\relax }}{25}{Optimization of the neural network population (DE-rand)\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Exploration and exploitation}{25}{paragraph*.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Overall population gets fitter (Mean, DE-rand)\relax }}{26}{figure.caption.46}}
\newlabel{malignant-mean-DE-rand}{{\M@TitleReference {5.2}{Overall population gets fitter (Mean, DE-rand)\relax }}{26}{Overall population gets fitter (Mean, DE-rand)\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Convergence towards optimal solution (DE-rand)\relax }}{26}{figure.caption.47}}
\newlabel{malignant-variance-DE-rand}{{\M@TitleReference {5.3}{Convergence towards optimal solution (DE-rand)\relax }}{26}{Convergence towards optimal solution (DE-rand)\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Average population fitness on increasingly large populations (DE-rand)\relax }}{27}{figure.caption.48}}
\newlabel{malignant-increasing-pop-size}{{\M@TitleReference {5.4}{Average population fitness on increasingly large populations (DE-rand)\relax }}{27}{Average population fitness on increasingly large populations (DE-rand)\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/BEST)}{28}{subsection*.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Optimization of the neural network population (DE-best)\relax }}{28}{figure.caption.50}}
\newlabel{malignant-perfs-DE-best}{{\M@TitleReference {5.5}{Optimization of the neural network population (DE-best)\relax }}{28}{Optimization of the neural network population (DE-best)\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Convergence towards optimal solution (DE-best)\relax }}{28}{figure.caption.51}}
\newlabel{malignant-variance-DE-best}{{\M@TitleReference {5.6}{Convergence towards optimal solution (DE-best)\relax }}{28}{Convergence towards optimal solution (DE-best)\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{Particle Swarm Optimization}{29}{subsection*.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Optimization of the neural network population (PSO)\relax }}{29}{figure.caption.53}}
\newlabel{malignant-perfs-PSO}{{\M@TitleReference {5.7}{Optimization of the neural network population (PSO)\relax }}{29}{Optimization of the neural network population (PSO)\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Convergence towards sub-optimal solution (PSO)\relax }}{30}{figure.caption.54}}
\newlabel{malignant-variance-PSO}{{\M@TitleReference {5.8}{Convergence towards sub-optimal solution (PSO)\relax }}{30}{Convergence towards sub-optimal solution (PSO)\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Overall population gets fitter (Mean, PSO)\relax }}{30}{figure.caption.55}}
\newlabel{malignant-mean-PSO}{{\M@TitleReference {5.9}{Overall population gets fitter (Mean, PSO)\relax }}{30}{Overall population gets fitter (Mean, PSO)\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Breast Cancer Recurrence}{31}{section.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Haberman's survival}{31}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Using a Derived Termination Criteria}{31}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Convergence towards optimal solution (DE derived termination criteria)\relax }}{32}{figure.caption.56}}
\newlabel{malignant-perfs-DE-derived-rand}{{\M@TitleReference {5.10}{Convergence towards optimal solution (DE derived termination criteria)\relax }}{32}{Convergence towards optimal solution (DE derived termination criteria)\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Convergence towards optimal solution (DE derived termination criteria)\relax }}{32}{figure.caption.57}}
\newlabel{malignant-variance-DE-derived-rand}{{\M@TitleReference {5.11}{Convergence towards optimal solution (DE derived termination criteria)\relax }}{32}{Convergence towards optimal solution (DE derived termination criteria)\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Overall population gets fitter (Mean, DE derived termination criteria)\relax }}{33}{figure.caption.58}}
\newlabel{malignant-mean-DE-derived-rand}{{\M@TitleReference {5.12}{Overall population gets fitter (Mean, DE derived termination criteria)\relax }}{33}{Overall population gets fitter (Mean, DE derived termination criteria)\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Measurement Methodology}{34}{section.5.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Experiment Replicability}{34}{section.5.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Seed settings\relax }}{34}{algorithm.4}}
\newlabel{pseudocode-seed-settings}{{\M@TitleReference {4}{Seed settings\relax }}{34}{Seed settings\relax }{algorithm.4}{}}
\citation{Gorunescu-2014}
\citation{Gorunescu-2014}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Comparison with previous work}{35}{section.5.7}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Results against state-of-the-art methods on Breast Cancer Wisconsin (Diagnostic). Source: \cite  {Gorunescu-2014}\relax }}{35}{table.caption.59}}
\newlabel{table:state-of-the-art-comparison}{{\M@TitleReference {5.1}{Results against state-of-the-art methods on Breast Cancer Wisconsin (Diagnostic). Source: \cite  {Gorunescu-2014}\relax }}{35}{Results against state-of-the-art methods on Breast Cancer Wisconsin (Diagnostic). Source: \cite {Gorunescu-2014}\relax }{table.caption.59}{}}
\citation{uci-machine-learning-repo-2013}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Evaluation}{36}{chapter.6}}
\citation{Gorunescu-2014}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Conclusions}{37}{chapter.7}}
\citation{stanley-2002}
\citation{yao-1999}
\citation{yao-1999}
\citation{stanley-2002}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Further Study}{38}{section.7.1}}
\@writefile{toc}{\contentsline {subsection}{Speciation}{38}{subsection*.60}}
\@writefile{toc}{\contentsline {subsection}{Hybrid optimization algorithms}{38}{subsection*.61}}
\@writefile{toc}{\contentsline {subsection}{Fitness penalties to guide Differential Evolution}{38}{subsection*.62}}
\@writefile{toc}{\contentsline {subsection}{Neural Network Ensembles}{38}{subsection*.63}}
\@writefile{toc}{\contentsline {subsection}{Comparing Genetic Algorithm and Artificial Immune Systems as optimization algorithms}{38}{subsection*.64}}
\@writefile{toc}{\contentsline {subsection}{Comparison with the NEAT method}{38}{subsection*.65}}
\@writefile{toc}{\contentsline {subsection}{Coevolution of topology and weight of Multitask neural networks}{39}{subsection*.66}}
\@writefile{toc}{\contentsline {subsection}{Investigation on a wider range of problems}{39}{subsection*.67}}
\citation{uci-machine-learning-repo-2013}
\citation{uci-machine-learning-repo-2013}
\citation{uci-machine-learning-repo-2013}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Data Set resources}{40}{appendix.A}}
\newlabel{App:AppendixA-data set-resources}{{\M@TitleReference {A}{Data Set resources}}{40}{Data Set resources}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{Data quality}{40}{subsection*.68}}
\@writefile{toc}{\contentsline {subsubsection}{Breast Cancer Wisconsin (Diagnostic)}{40}{subsubsection*.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Instances of the breast cancer data set. Visualisation of common-features with identical severity\relax }}{40}{figure.caption.69}}
\newlabel{breast-cancer-visualization}{{\M@TitleReference {A.1}{Instances of the breast cancer data set. Visualisation of common-features with identical severity\relax }}{40}{Instances of the breast cancer data set. Visualisation of common-features with identical severity\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Breast cancer diagnosis data set description\relax }}{40}{figure.caption.69}}
\newlabel{breast-cancer-diagnosis-data set-description}{{\M@TitleReference {A.2}{Breast cancer diagnosis data set description\relax }}{40}{Breast cancer diagnosis data set description\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsubsection}{Haberman's survival}{41}{subsubsection*.72}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces  Haberman's survival data set description\relax }}{41}{figure.caption.71}}
\newlabel{ haberman-data set-description}{{\M@TitleReference {A.3}{ Haberman's survival data set description\relax }}{41}{Haberman's survival data set description\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{Breast Cancer: Recurrence or Not}{41}{subsubsection*.74}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Data set description: Breast Cancer: Malignant or Benign\relax }}{41}{figure.caption.73}}
\newlabel{mammographic-mass-screaning-data set-description}{{\M@TitleReference {A.4}{Data set description: Breast Cancer: Malignant or Benign\relax }}{41}{Data set description: Breast Cancer: Malignant or Benign\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Legend of Figure \ref  {breast-cancer-visualization}\relax }}{41}{figure.caption.75}}
\newlabel{breast-cancer-visualization-legend}{{\M@TitleReference {A.5}{Legend of Figure \ref  {breast-cancer-visualization}\relax }}{41}{Legend of Figure \ref {breast-cancer-visualization}\relax }{figure.caption.75}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {B}Results - Breast Cancer: Recurrence or Not ?}{42}{appendix.B}}
\newlabel{complementary-results-recurrence}{{\M@TitleReference {B}{Results - Breast Cancer: Recurrence or Not ?}}{42}{Results - Breast Cancer: Recurrence or Not ?}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/RAND)}{42}{subsection*.76}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Optimization of the neural network population (DE/rand)\relax }}{42}{figure.caption.77}}
\newlabel{recurrence-perfs}{{\M@TitleReference {B.1}{Optimization of the neural network population (DE/rand)\relax }}{42}{Optimization of the neural network population (DE/rand)\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Convergence towards optimal solution\relax }}{42}{figure.caption.78}}
\newlabel{recurrence-variance}{{\M@TitleReference {B.2}{Convergence towards optimal solution\relax }}{42}{Convergence towards optimal solution\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/BEST)}{43}{subsection*.79}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Learning pace\relax }}{43}{figure.caption.80}}
\newlabel{recurrence-perfs}{{\M@TitleReference {B.3}{Learning pace\relax }}{43}{Learning pace\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Convergence towards optimal solution\relax }}{43}{figure.caption.81}}
\newlabel{recurrence-variance}{{\M@TitleReference {B.4}{Convergence towards optimal solution\relax }}{43}{Convergence towards optimal solution\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Mean of the scores of the individuals\relax }}{44}{figure.caption.82}}
\newlabel{recurrence-mean-DE}{{\M@TitleReference {B.5}{Mean of the scores of the individuals\relax }}{44}{Mean of the scores of the individuals\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{Particle Swarm Optimization}{45}{subsection*.83}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Optimization of the neural network population (PSO)\relax }}{45}{figure.caption.84}}
\newlabel{recurrence-perfs-PSO}{{\M@TitleReference {B.6}{Optimization of the neural network population (PSO)\relax }}{45}{Optimization of the neural network population (PSO)\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Mean of the scores of the individuals (PSO)\relax }}{45}{figure.caption.85}}
\newlabel{recurrence-mean-PSO}{{\M@TitleReference {B.7}{Mean of the scores of the individuals (PSO)\relax }}{45}{Mean of the scores of the individuals (PSO)\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Optimization of the neural network population (PSO)\relax }}{46}{figure.caption.86}}
\newlabel{recurrence-perfs-PSO}{{\M@TitleReference {B.8}{Optimization of the neural network population (PSO)\relax }}{46}{Optimization of the neural network population (PSO)\relax }{figure.caption.86}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {C}Haberman's survival test}{47}{appendix.C}}
\newlabel{complementary-results-haberman}{{\M@TitleReference {C}{Haberman's survival test}}{47}{Haberman's survival test}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/RAND/1)}{47}{subsection*.87}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Optimization of the neural network population (DE/rand)\relax }}{47}{figure.caption.88}}
\newlabel{haberman-perfs-DE-rand}{{\M@TitleReference {C.1}{Optimization of the neural network population (DE/rand)\relax }}{47}{Optimization of the neural network population (DE/rand)\relax }{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Convergence towards optimal solution\relax }}{47}{figure.caption.89}}
\newlabel{haberman-variance-DE-rand}{{\M@TitleReference {C.2}{Convergence towards optimal solution\relax }}{47}{Convergence towards optimal solution\relax }{figure.caption.89}{}}
\@writefile{toc}{\contentsline {subsection}{Differential Evolution (mutation scheme: DE/BEST/1)}{48}{subsection*.90}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces Optimization of the neural network population (DE/best)\relax }}{48}{figure.caption.91}}
\newlabel{haberman-perfs-de-rand}{{\M@TitleReference {C.3}{Optimization of the neural network population (DE/best)\relax }}{48}{Optimization of the neural network population (DE/best)\relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Convergence towards optimal solution\relax }}{48}{figure.caption.92}}
\newlabel{haberman-variance-de-rand}{{\M@TitleReference {C.4}{Convergence towards optimal solution\relax }}{48}{Convergence towards optimal solution\relax }{figure.caption.92}{}}
\@writefile{toc}{\contentsline {subsection}{Particle Swarm Optimization}{49}{subsection*.93}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.5}{\ignorespaces Optimization of the neural network population (PSO)\relax }}{49}{figure.caption.94}}
\newlabel{haberman-perfs-PSO}{{\M@TitleReference {C.5}{Optimization of the neural network population (PSO)\relax }}{49}{Optimization of the neural network population (PSO)\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.6}{\ignorespaces Mean of the scores of the individuals (PSO)\relax }}{49}{figure.caption.95}}
\newlabel{haberman-mean-PSO}{{\M@TitleReference {C.6}{Mean of the scores of the individuals (PSO)\relax }}{49}{Mean of the scores of the individuals (PSO)\relax }{figure.caption.95}{}}
\bibdata{latex-citations}
\bibcite{bishop-1995}{1}
\bibcite{chilimbi-2014}{2}
\bibcite{eberhart-1995}{3}
\bibcite{Gorunescu-2014}{4}
\bibcite{haupt-1998}{5}
\bibcite{haykin-1998}{6}
\bibcite{huang-2013}{7}
\bibcite{yaochu-2004}{8}
\bibcite{lecun-data-driven-2014}{9}
\bibcite{lecun-convolutional-2010}{10}
\bibcite{uci-machine-learning-repo-2013}{11}
\bibcite{tereza-2006}{12}
\bibcite{mccormick-2013}{13}
\bibcite{negnevitsky-2011}{14}
\bibcite{coursera-machine-learning-stanford}{15}
\bibcite{deeplearning4j}{16}
\bibcite{sibi-2013}{17}
\bibcite{fortmann-2012}{18}
\bibcite{schaffer-1990}{19}
\bibcite{hassabis-2016}{20}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{50}{section*.97}}
\bibcite{floreano-2008}{21}
\bibcite{stanley-2002}{22}
\bibcite{storn-1997}{23}
\bibcite{waghulde-2014}{24}
\bibcite{welch-labs}{25}
\bibcite{wiki-artificial-neuron}{26}
\bibcite{wikipedia-evolutionary-algorithms}{27}
\bibcite{yao-1999}{28}
\bibcite{zhang-2000}{29}
\bibstyle{plain}
\memsetcounter{lastsheet}{51}
\memsetcounter{lastpage}{51}
